[
    {
        "title": "Variant Co-Occurrence (Phasing) Information in gnomAD",
        "link": "https://gnomad.broadinstitute.org/news/2021-07-variant-co-occurrence-phasing-information-in-gnomad/",
        "date": "2021-08-12",
        "text": "Today, we are pleased to announce the incorporation of variant co-occurrence (inferred phasing) information in the gnomAD v2 browser. Phase refers to the genetic relationship between a pair of variants; that is, whether the variants are on the same copy of the gene (\ncis\n) or on different copies of the gene (\ntrans\n). We are releasing inferred phasing data for all pairs of variants within a gene where both variants have a global allele frequency in gnomAD exomes <5% and are either coding, flanking intronic (from position -1 to -3 in acceptor sites, and +1 to +8 in donor sites) or in the 5’/3’ UTRs. This encompasses 20,921,100 pairs of variants across 19,685 genes. We envision that this data will be of tremendous help to the medical genetics community in identifying and interpreting co-occurring variants in the context of recessive conditions.\nBackground\nA pair of variants in a gene can occur in\ncis\n(same copy of the gene) or\ntrans\n(different copies of the gene). This distinction has important consequences for interpreting the functional effect of a pair of variants. For an autosomal recessive disorder, both copies of a gene must have pathogenic variants in order to develop disease (i.e., occur in\ntrans\n). This is illustrated in the example below. In these two example trios, the father’s transmitted copy of the gene is in solid blue, and the mother’s transmitted copy of the gene is in solid yellow. On the left, the child inherits one pathogenic variant from the father and one pathogenic variant from the mother. As both copies of the gene have pathogenic variants, the child is at risk for developing the recessive disorder. On the right, the child inherits both variants from the father, and because only one copy of the gene is carrying the variants, this is insufficient to develop a recessive disorder.\nHowever, based on the child’s DNA sequencing alone, these two scenarios are indistinguishable in the vast majority of scenarios when short read next generation sequencing data is used. This is because most pairs of variants are further apart from each other than the length of a single sequencing read and thus their phase cannot be determined based on the child’s sequencing data itself. (See the\n“Multinucleotide Variants”\nsection below for more details on how we handle coding variants that occur within 3 bp.) To overcome this challenge, clinical geneticists often sequence parental DNA. If the two variants are in\ncis\n, they should be both present in one parent. If they are in\ntrans\n, each variant will be seen in a different parent. However, sequencing parental DNA increases cost, and parental DNA is not always available. Occasionally, we can apply long read sequencing to help infer phase, but this is still quite expensive to perform.\nThere are a number of accurate and efficient algorithms for determining phase for variants that are common in the population. However, phasing is not trivial for rare variants, which are the variants that are likely to be of greatest importance in Mendelian conditions. Additionally, exome sequencing data, in which the vast majority of the genome is not covered, does not provide enough density of surrounding variants to allow for accurate phasing. Phasing is also computationally intensive and can be technically challenging. Finally, phasing requires full variant data (i.e., more than what is available in a clinical genetic report), which is often not available to end users.\nApproach\nHere, we take a different approach to tackling these challenges. The genetic relationship between two variants (phase), is shared across individuals in a population. In other words, if two variants are in\ncis\nin many individuals in a population, then they are likely to be\ncis\nin a given individual’s DNA. Similarly, if two variants are in\ntrans\nin other individuals in a population, then they are likely to be\ntrans\nin a given individual’s DNA. These relationships are only disrupted by one of two processes:\nRecombination between variants. However, if variants are in close physical proximity to each other (as is true for most variants within a gene), then rates of recombination are low.\nRecurrent mutation. If variants arise independently multiple times in a population’s history, then the genetic relationships between them are not reliable.\nGiven that phase is shared among individuals in a population, we reasoned that if we could determine the phase between pairs of variants in gnomAD, this would allow users to predict these relationships for their samples.\nAfter experimenting with a number of algorithms for phasing variants in gnomAD, we settled on a decades-old Expectation-Maximization (E-M) algorithm (\nExcoffier and Slatkin\n). We found that the phasing from the E-M algorithm was accurate, computationally tractable, and conducive to interpretation. The core idea behind this algorithm is that if a pair of variants are frequently or always seen together in the same individual in a population (i.e., co-occur), then they likely exist on the same haplotype. In contrast, if the two variants are rarely or never seen together in the same individual in a population, then they likely exist on different haplotypes.\nWe then determined the phase of every pair of variants within each gene in gnomAD, where both variants have a global MAF <5% and are either coding, flanking intronic (from position -1 to -3 in acceptor sites, and +1 to +8 in donor sites), or in the 5’/3’ UTR. We selected these cutoffs to reduce the computational and storage burden of this data, while focusing on variants that are likely to be of greatest importance in medical genetics. We performed these calculations separately for each continental ancestral population in gnomAD, as well as in aggregate across all gnomAD samples.\nWe validated this approach using 4,992 trios, leveraging the fact that we can uniquely phase variants using trios as a “truth” set. For this experiment, we removed individuals in gnomAD that were part of or related to the individuals in the trios. We found that we are 92-99% accurate for variants when they are on the same haplotype (\ncis\n) and 90-97% accurate for variants when they are on different haplotypes (\ntrans\n), depending on the population. This remains the case even for extremely rare variants (<0.5% MAF), where we see similar accuracies. Phasing accuracy for pairs of variants that include singletons was generally lower (81-97%).\nDifferent haplotypes\nSame haplotypes\nPopulation\nAF<5%\nAF<0.5%\nSingleton\nAF<5%\nAF<0.5%\nSingleton\nAfrican/African American\n0.928\n0.960\n0.853\n0.943\n0.962\n0.853\nLatino/Admixed American\n0.928\n0.952\n0.893\n0.956\n0.958\n0.893\nAshkenazi Jewish\n0.967\n0.992\n0.965\n0.992\n0.994\n0.965\nEast Asian\n0.896\n0.947\n0.897\n0.942\n0.949\n0.897\nEuropean (Finnish)\n0.963\n0.984\n0.960\n0.988\n0.990\n0.960\nEuropean (non-Finnish)\n0.896\n0.908\n0.811\n0.924\n0.922\n0.811\nSouth Asian\n0.954\n0.947\n0.841\n0.963\n0.950\n0.841\nAll\n0.774\n0.780\n0.803\n0.833\n0.805\n0.803\nHow to use the browser\nOn the variant co-occurrence\npage\n, users can input the information for any pair of variants within a single gene using a chr-pos-ref-alt format based on hg19/GRCh37 coordinates. The browser may return one of three possibilities:\nAn error if one or both of your variants are missing in gnomAD. In this scenario, we are unfortunately unable to provide phasing information.\nAn error if any of the following conditions hold: the variants are not in the same gene; at least one variant occurs at a frequency >5%; at least one variant does not occur in the supported regions (coding, flanking intronic, and 5’/3’ UTRs).\nA detailed output window, an example of which is shown below. Let’s use two variants (\n1-55505647-G-T and 1-55523855-G-A\n) in the gene\nPCSK9\nas an example. When you enter these two variants on the landing page, you will get the output page displayed below. On the top, you will see a table broken down by continental ancestry. For each ancestry, we list how many exomes in gnomAD from that ancestry are consistent with the two variants occurring on different haplotypes (\ntrans\n), and how many samples are consistent with their occurring on the same haplotype (\ncis\n). Below that, there is a 3x3 table that contains the 9 possible combinations of genotypes for the two variants of interest. The number of exomes in gnomAD that fall in each of these combinations are shown and are colored by whether they are consistent with variants falling on different haplotypes (red) or the same haplotype (blue), or whether they are indeterminate (purple). On the bottom right, you will see the estimated haplotype counts for the four possible haplotypes for your two variants. These are the haplotype counts estimated using the EM algorithm, and they allow us to predict whether your variants are more likely to be on the same or different haplotypes.\nIn this example, the two variants are predicted to be on different haplotypes in most populations. If you click on a population (e.g., East Asian), the values in the Genotype Counts table are updated to reflect counts in the specified population. In this case, one of the variants (1-55505647-G-T) does not appear in any East Asian individuals, so no prediction can be made.\nBased on the haplotype estimates, we also calculate probabilities that a pair of variants are on the same or different haplotypes. The overall distribution of the probability of each variant pair occurring on different haplotypes is shown below for reference, with most variant pairs falling at the extreme ends of the distribution.\nWe caution users not to overinterpret the provided probabilities for whether variants are more likely on the same or different haplotypes.\nThese are not well calibrated and often based on small numbers of exomes, and we intentionally do not assign statistical significance to these numbers.\nMultinucleotide Variants\nThis isn’t the first time that we have looked at patterns of co-occurrence in gnomAD data — we have\npreviously published\non multinucleotide variants (MNVs), which are cases where multiple variants are present close together on the same haplotype. For these variants we are able to use a more accurate approach to determine co-occurrence, taking advantage of the fact that variants that are present in the same read must be on the same haplotype (termed\nread-backed phasing\n). Our browser uses this approach to annotate and flag nearby (<3 bp) coding variant pairs in the same codon in\ncis\n, as these can have a distinct effect on the protein sequence relative to the individual variants. Because the read-backed phasing approach is more accurate than the statistical phasing approach discussed above, we recommend that users rely on MNV annotations if those are available. We have added a link to the MNV page for pairs of variants where this information is available.\nCaveats and limitations\nA key limitation to our current approach is that many Mendelian disease variants are exceedingly rare in the general population, and phasing accuracy for extremely rare variants is generally lower. Additionally, variants in a given individual are most accurately phased when based on data from the corresponding ancestral population. However, the sample sizes for some ancestral populations in gnomAD are still relatively small, limiting the accuracy of the phasing for these populations. As additional, ancestrally diverse samples are added in future gnomAD releases, we will be able to update our calculations to improve our accuracy; and we will also be able to phase a greater number of variant pairs.\nThe accuracy of phasing also depends on recombination and mutation rates, as alluded to above. If there is a recombination hotspot between a pair of variants, or they are far apart (as can happen in a very long gene), then recombination can disrupt the genetic relationships and render this approach less accurate. Also, if a variant is recurrently mutating (and thus arising recurrently on multiple haplotypes), our approach will be less accurate.\nConclusion\nIn summary, we leverage the size of gnomAD to provide a new resource to the medical genetics community on the phase of rare variants. We hope that the variant co-occurrence information now displayed in the gnomAD browser will greatly aid in the interpretation of co-occurring variants, and we encourage users to write to\ngnomad@broadinstitute.org\nwith comments and suggestions for improving this feature."
    },
    {
        "title": "gnomAD v3.1 Mitochondrial DNA Variants",
        "link": "https://gnomad.broadinstitute.org/news/2020-11-gnomad-v3-1-mitochondrial-dna-variants/",
        "date": "2020-11-17",
        "text": "Overview\nMitochondrial DNA (mtDNA) variants for gnomAD are now\navailable\nfor the first time! We have called mtDNA variants for 56,434 whole genome samples in the v3.1 release. This initial release includes population frequencies for 10,850 unique mtDNA variants defined at more than half of all mtDNA bases. The vast majority of variant calls (98%) are homoplasmic or near homoplasmic, whereas 2% are heteroplasmic. Variation in mitochondrial genomes contributes to many human diseases and has had unique value in the study of human evolutionary genetics. We hope that the addition of mtDNA to gnomAD will enable researchers to better understand the role of mtDNA variation in both health and disease states.\nPrevious gnomAD callsets have not included mtDNA variants because their properties do not fit the assumptions that we use with our nuclear variant calling pipeline. These properties include:\nA circular genome: Alignment of reads from the circular mtDNA to its linear representation in the human reference assembly can cause mapping problems.\nHeteroplasmy: Cells each contain hundreds to thousands of mtDNA copies. Most variants are homoplasmic, meaning they are present in all mtDNA molecules in a cell, and represent differences between the given individual and the reference human mitochondrial genome. However, some variants are heteroplasmic, or present in only a fraction of the cell’s mtDNA molecules. A key challenge is to call heteroplasmic variants (particularly at low fractions) and to distinguish low heteroplasmy variants from technical artifacts or contamination.\nNUMTs: Nuclear sequences of mitochondrial origin (termed “NUMTs”) are derived from pieces of mtDNA that have integrated into the nuclear genome over the course of human evolution. Many NUMTs are part of the reference human genome assembly; however, polymorphic NUMTs exist that are present only in some individuals. Reads derived from NUMTs often mis-align to the mtDNA and generate false positive calls at low heteroplasmy. Conversely, reads genuinely arising from the mtDNA genome can be mis-aligned to the NUMTs in the reference genome.\nTo address these issues, we developed a new mtDNA calling pipeline and used it to create the gnomAD v3.1 mtDNA callset. Our mtDNA data include allele frequencies for heteroplasmic and homoplasmic variants, haplogroup-specific allele frequencies, predicted consequences for protein and tRNA variants, and links to mitochondrial resources.\nCreating the v3.1 mtDNA Callset\nmtDNA Calling Pipeline for Single Samples\nWe developed a mitochondria mode of GATK MuTect2 to call variants in GRCh38 chrM (identical to the revised Cambridge Reference Sequence, rCRS, GenBankNC_012920.1) on individual samples. This pipeline, which can be run on the\nTerra platform\n, addresses challenges specific to calling mtDNA variants.\nCircular genome: The mtDNA non-coding “control region” spans the artificial break in the circular genome (coordinates chrM:16024-16569 and chrM:1-576), which can make it challenging to call variants in this region. Therefore to call control region variants, we extracted all chrM reads and realigned them to a mitochondrial reference that was shifted by 8,000 bases, called variants on this shifted alignment, and then converted coordinates back to their original positions.\nHeteroplasmic variants: Unlike nuclear variants, which are called as either heterozygous or homozygous, mtDNA variants are called at all heteroplasmy levels using a specialized mitochondria mode of the GATK MuTect2 variant caller (a variant caller that was originally designed to call somatic variants in tumor samples). MuTect2 calls are then stringently filtered as described below.\nHaplogroups: mtDNA does not recombine and is inherited maternally. Closely related mtDNA sequences have historically been grouped together in “haplogroups.” There are over 5000 haplogroups from diverse populations available in the\nPhylotree database\n. The pipeline annotates a specific haplogroup for each sample using\nHaplogrep\n.\nWe would like to note that we have tested our pipeline only on WGS samples. Because many exome capture technologies specifically block mtDNA reads from being sequenced, many exomes lack sufficient mtDNA coverage to accurately call variants, particularly those with low heteroplasmy.\nmtDNA Calling Pipeline for the gnomAD Callset\nTo generate the gnomAD mtDNA callset, we combined single sample VCFs and applied stringent quality control filters for samples, variants, and genotypes as described below. Specifically, we excluded samples with low or high mtDNA copies/cell or with evidence of mtDNA contamination. For this release we also filtered out variants with heteroplasmy < 10% in order to avoid false positives derived from contamination, sequencing errors, and NUMT misalignment. Subsequent releases will include variants below 10% heteroplasmy that pass additional QC filters.\nGenome Variant Calls\nThe\npipeline\ndescribed above was run on individual samples using the Terra platform. Homoplasmic reference calls were assigned to non-variant sites with coverage > 100X, whereas non-variant sites with coverage <= 100X were labeled as missing data and not used for calculation of population allele frequencies.\nIn contrast to the notation for nuclear homozygous and heterozygous genotypes, the mtDNA VCF file denotes homoplasmic genotypes as “1/1,” indicating 95-100% alternate bases, and heteroplasmic genotypes as “0/1,” indicating < 95% alternative bases. Calls below 1% heteroplasmy were completely removed. For the initial v3.1 release, a “heteroplasmy_below_10_percent” genotype filter was applied to calls below 10% heteroplasmy.\nSample Exclusion Criteria\nThe samples included in the mtDNA callset are a subset of the samples available for the nuclear genome in gnomAD v3.1. We implemented several filters to remove low-quality samples:\nLow or high mtDNA copy number: Samples with an estimated mtDNA copy number less than 50 were removed (N=6,505 samples) since they are prone to contamination and NUMT-derived false positive calls. Mitochondrial copy number was calculated as 2*mean mitochondrial coverage/median autosomal coverage. Samples with a mitochondrial copy number greater than 500 were also removed (N=5,633 samples) because we observed that these samples are more likely to originate from cell lines, which contain higher numbers of cell culture related heteroplasmies likely due to somatic mutations and selection. DNA source information (i.e. blood, saliva, cell line) is not routinely available for samples in gnomAD.\nMitochondrial contamination: 1,803 samples were removed if mtDNA contamination estimates exceeded 2%, based on the maximum from nuclear contamination (VerifyBamID freemix value), mtDNA haplogroup contamination (Haplocheck), or an internal algorithm to flag contamination or NUMT misalignment. This internal algorithm estimates contamination by examining haplogroup markers that should show 100% alternative alleles (i.e., are fully homoplasmic) but are observed at high heteroplasmy instead (85-99.8% alternate alleles). It is defined for each sample as one minus the mean heteroplasmy of any haplogroup-defining variants observed with heteroplasmy (85-99.8% alternate alleles) if at least 3 such variants are present; otherwise estimated contamination is defined as one minus the mean heteroplasmy of haplogroup-defining variants with heteroplasmy 85-100%.\nAccess to read data: We excluded 5,781 gnomAD v3.1 samples for which we no longer had access to read data.\nThe pipeline for mtDNA callset assembly and annotation was implemented in\nHail\n.\nSite-level Filters and Flags\nAll alleles at certain problematic sites were filtered out or flagged.\nartifact_prone_site\n(filter): This is one of 6 specific mtDNA positions (301, 302, 310, 316, 3107, 16182) where sequence context makes it difficult to distinguish true variants from technical artifacts, and therefore all variants overlapping these sites are filtered out. The homopolymer tracts at location chrM:300-317 (AAACCCCCCCTCCCCCGC) cause Illumina sequencing errors in all samples and cause (i) a large coverage dip in this region, (ii) reads with many apparent indels near position chrM:310T, and (iii) apparent substitutions of chrM:301A>C, chrM:302A>C, chrM:310T>C, and chrM:316G>C. Similarly, homopolymer tracts at location chrM:16180-16193 (AAAACCCCCTCCCC) cause errors and apparent indels at position chrM:16182. The reference genome contains “N” at position chrM:3107, which causes misalignment of many reads.\nindel_stack\n(filter): Similar to artifact-prone sites, certain indels create a homopolymer tract that causes a drop in coverage and technical sequencing artifacts in multiple individuals. For example, an individual with an insertion at position chrM:5892 would typically show multiple alternate alleles (e.g., REF=T, ALT=TC, TCC, TCCC, TCCCCC, TCCCCC, TCCCCCCCC), which represents a multi-allelic call in this sample. Indels that are only present within multi-allelic calls across all samples in the callset are filtered out using this flag. For example, of the 182 different indel variants observed at position chrM:5892, 102 are only detected within multi-allelic calls and are filtered out as\nindel_stack\n, whereas alternate variants such as chrM:5892T>TC and chrM:5892T>TCC are not always in multi-allelic calls and will pass filters.\nnpg\n(filter): No sample had a pass genotype for the variant.\ncommon_low_heteroplasmy\n(flag): This flag is present if the variant is found at an overall frequency of > 0.001 across all samples with a PASS genotype and heteroplasmy level > 0 and < 50% (includes variants < 1% heteroplasmy which are subsequently filtered). Variants that are common at low heteroplasmy are likely to be enriched for sequencing errors and NUMT misalignments; they may also represent recurrent somatic mutations.\nGenotype-level Filters\nGenotype filters are applied separately to each genotype in each sample. Only variants that pass genotype filters are displayed by default, although specialized searches (i.e., searching for the particular variants or toggling the “Filtered variants” checkbox) can allow users to view variants in which all genotypes were filtered. The number of filtered genotypes is reported in the excluded allele count (“excluded_AC”), and a histogram is available on variant pages to view the counts of specific filters across different heteroplasmy levels, but these genotypes are not used for allele count and allele frequency calculations. Low heteroplasmic calls should be treated with caution and may be enriched with artifacts, but we want to make information on these calls available to benefit the community and allow users to put their own calls in context, such as to evaluate potential shared error modes.\nbase_qual\n: Median base quality of alternate allele was below minimum (using default of 20 for “min-median-base-quality” parameter)\nheteroplasmy_below_10_percent\n: Heteroplasmy level was below 10%\nposition\n: Median distance of variant allele from end of reads was below minimum (using default of 1 for “\nmin-median-read-position\n” parameter)\nstrand_bias\n: Evidence for alternate allele comes from one read direction only\nweak_evidence\n: Mutation does not meet likelihood threshold\ncontamination\n: Fails MuTect2 contamination filter based on Haplocheck (does not take into account the freemix value or our internal algorithm for calculating contamination)\nVariant Annotations\nTo make the mtDNA data more usable, we provide multiple variant annotations on the website and in the downloadable callset. Some of these are listed below.\nWe do not calculate an overall allele count and allele frequency. Instead, separate allele counts and frequencies are provided for both homoplasmic and heteroplasmic variants as well as for each top-level haplogroup. For example, the “AF_hom” annotation provides the overall allele frequency taking into account only variants that were called homoplasmic in samples with that variant (heteroplasmy 95-100%).\nWe annotate variants that PhyloTree Build 17 defines as homoplasmic within at least one haplogroup with the flag, “hap_defining_variant.”\nIn-silico prediction annotations for tRNA variants were obtained from\nPON-mt-tRNA\n,\nMITOMAP\n, and\nHmtVar\n.\nVEP annotations are provided using the VEP annotation pipeline for nuclear variants with the following modification: the VEP distance parameter was set to 0 to avoid “upstream” and “downstream” annotations.\nWe edited some annotations generated by LOFTEE since the assumptions for the nuclear genome do not apply to the mtDNA: (i) the “SINGLE_EXON” flag was removed, as all mtDNA transcripts are single exon; (ii) “LC” (low confidence) loss-of-function variants due to “END_TRUNC” were edited to “HC” (high confidence), again because all mtDNA transcripts are short, single exon genes not subject to nonsense mediated decay; and (iii) the “END_TRUNC” filter was removed. All other annotations with LOFTEE remain unchanged, and caution should be taken to interpret the annotations in the context of the mitochondrial genome.\nDownloads\nfor mtDNA variants are provided separately from the nuclear variants.\nAuthors\nKristen Laricchia, Nicole Lake, Nick Watts, Megan Shand, Andrea Haessly, Laura Gauthier, David Benjamin, Eric Banks, Jose Soto, James Emery, Grace Tiao, Daniel MacArthur, Monkol Lek, Vamsi K. Mootha, Sarah E. Calvo\nAuthor Contributions\nThis project was a major collaborative effort. Kristen Laricchia was the lead analyst. Sarah Calvo provided leadership, expert knowledge, and detailed examination of the data. Nicole Lake conducted analyses to aid in quality control. David Benjamin developed the mitochondria mode of MuTect2. Megan Shand and Andrea Haessly developed the mitochondrial pipeline and WDL with guidance from Laura Gauthier and Eric Banks and assistance from Jose Soto and James Emery. Nick Watts adapted the gnomAD browser to incorporate mitochondria-specific annotations. Grace Tiao provided oversight of the collaboration. Daniel MacArthur, Monkol Lek, and Vamsi Mootha provided leadership and analytical advice.\nAcknowledgments\nWe thank Heidi Rehm and Mark Daly for gnomAD leadership; Sebastian Schoenherr for assistance in understanding Haplocheck; Konrad Karczewski for reviewing the code for callset assembly and annotation; Kiran Garimella for assistance in understanding NUMTs with long-read data; David Thorburn, John Christodoulou, and members of their labs for their feedback on displaying mtDNA variants on the browser. This work was supported in part by a Broad Institute Scientific Projects to Accelerate Research and Collaboration (SPARC) grant."
    },
    {
        "title": "gnomAD v3.1 New Content, Methods, Annotations, and Data Availability",
        "link": "https://gnomad.broadinstitute.org/news/2020-10-gnomad-v3-1-new-content-methods-annotations-and-data-availability/",
        "date": "2020-10-29",
        "text": "We’re proud to announce the gnomAD v3.1 release of 759,302,267 short nuclear variants (644,267,978 passing variant quality filters) observed in 76,156 genome samples.\nIn this release, we have included more than 3,000 new samples specifically chosen to increase the ancestral diversity of the resource. As a result, this is the first release for which we have a designated population label for samples of Middle Eastern ancestry, and we are thrilled to be able to include these in the following population breakdown for the v3.1 release:\nPopulation\nDescription\nGenomes\nafr\nAfrican/African American\n20,744\nami\nAmish\n456\namr\nLatino/Admixed American\n7,647\nasj\nAshkenazi Jewish\n1,736\neas\nEast Asian\n2,604\nfin\nFinnish\n5,316\nnfe\nNon-Finnish European\n34,029\nmid\nMiddle Eastern\n158\nsas\nSouth Asian\n2,419\noth\nOther (population not assigned)\n1,047\nTopics\nThe changes and new features in this release include:\nInnovation in incremental joint calling\nDocumentation of functions and code used to generate the callset\nIndividual-level genotypes available for HGDP + 1KG subsets\nRead visualizations in noncoding regions\nNew subset availability for frequency annotations\nNew in-silico prediction annotations\nTweaks and updates to VEP, dbSNP rsIDs, homozygous genotypes in common variants, and terminology for individuals’ chromosomal sex\nFully free access to downloadable datasets on multiple cloud providers\nWe’ll discuss each of these features in greater detail in the sections below and, at the end, we’ll share updated plots for v3.1\nsample and variant QC\n.\nIncremental joint-calling\nTo create gnomAD v3, the first version of this genome release, we took advantage of a new sparse (but lossless) data format developed by Chris Vittal and Cotton Seed on the Hail team to store individual genotypes in a fraction of the space required by traditional VCFs. In a previous blog\npost\ndescribing this innovation, we noted that one advantage of this new format was the possibility of appending new data to existing callsets without needing to re-process samples already joint called as part of prior gnomAD releases—effectively solving the “N+1” joint calling problem.\nFor gnomAD v3.1, we made good on this promise, adding 4,598 new genomes in gVCF form to the already extant, joint-called gnomAD v3 callset stored in the sparse Hail Matrix Table format. This is, to our knowledge, the first time that this procedure has been done. Chris Vittal added the new genomes for us in six hours—shaving off almost a week of compute time (or several million core hours) that would have been required if we had created the callset from scratch.\nThe total cost of the addition was about $400, or ~$0.09 per new sample and ~$0.004 per existing sample. By contrast, creating the callset from scratch would have cost ~$13,000.\nThe gnomAD Python package\nOver the last year, we’ve collectively poured thousands of hours into curating and fully\ndocumenting\na public code repository with Python functions we repeatedly use to produce large callsets, including but not limited to gnomAD. The gnomAD v3.1\nproduction pipeline\ndrew heavily on this resource, and we hope that others will benefit from it as well.\nThe package includes functions to help users handle sparse Matrix Tables, annotate variants with VEP, lift over sites from GRCh37 to GRCh38 (or vice versa), infer ancestry and cryptic relatedness within a callset, infer chromosomal sex, train and evaluate random forests variant filtering models, interact with linkage disequilibrium Block Matrices, export data to standard VCF format, and much more.\nUsers can access the entire repository with a simple command:\n$ pip install gnomad\nThe gnomAD HGDP and 1000 Genomes callset\nWe’re delighted to provide, for the first time,\ndownloads\nwith individual-level genotypes for a subset of gnomAD that is available to the public with no access restrictions. We hope this resource will be useful for a broad range of research applications — serving as a diverse reference panel for haplotype phasing and genotype imputation, for example, or as a training set for ancestry inference.\nThe samples included in this subset are drawn from the\n1000 Genomes Project\n(n=2,500) and the\nHuman Genome Diversity Project\n(n=780), which contain some of the most genetically diverse populations present in gnomAD. Collectively they represent human genetic diversity sampled across >60 distinct populations from Africa, Europe, the Middle East, South and Central and South Asia, East Asia, Oceania, and the Americas.\nTo create this callset, we re-processed raw data from the 1000 Genomes Project and HGDP to meet the\nfunctional equivalence\nstandard and joint-called the re-processed data with the rest of the gnomAD callset. (See “Incremental joint calling” above for more details on the joint-calling process.) The callset contains all high-quality samples (n=3,942) from these projects that passed gnomAD sample QC filters. These files therefore contain data for individuals (n=662) that we did not ultimately include in the gnomAD v3.1 aggregate allele frequencies, as these individuals were found to be second-degree related (or closer) to other individuals in 1000 Genomes, HGDP, or the larger gnomAD callset. We have provided a sample metadata table containing each sample’s status of inclusion or exclusion in the v3.1 release, along with sample-level quality control metrics and ancestry labels to accompany the VCF. (This information is directly annotated as column annotations in the Matrix Table version of the callset.)\nAll variants found in this cohort of samples are included, along with the variant filter status, metrics used to assess variant quality, and gnomAD v3.1 allele frequencies.\nAll called genotypes are included in this dataset, including low-quality genotypes that we filtered before computing gnomAD allele counts, allele numbers, and allele frequencies. To reproduce gnomAD genotype filtering, users should filter to genotypes with genotype quality (GQ) >= 20, depth (DP) >=10 (5 for haploid genotypes on sex chromosomes), and allele balance (AB) >= 0.2 and <= 0.8 (for heterozygous genotypes only). Users working with the Matrix Table version of the dataset can call the\nannotate_adj()\nfunction in the gnomAD Python package to annotate and filter genotypes:\n$ mt = annotate_adj(mt)\n$ mt = mt.filter_entries(mt.adj)\nThe code above adds an\nadj\n(“adjusted,” or passing) genotype annotation using the default gnomAD thresholds and then removes any genotype whose\nadj\nstatus is\nFalse\n.\nRead visualizations for non-coding regions\nSince our beginning as a project, we’ve made read data visualizations available for representative genotypes for most of the variants we observe in gnomAD exome data. Generating read visualizations for v3.1 whole genome data, however, has been considerably more challenging, as 99% of the genome is non-coding: there’s a lot more genomic territory to cover in whole-genome data.\nConsider the following numbers: to support non-coding read visualizations in this release, we generated over 2.5 billion read visualizations representing nearly 600 million unique variants observed in gnomAD v3.1. To be exact, we collected reads for 2,573,960,015 non-reference genotypes observed across 584,562,724 unique variants.\nWe needed a creative approach to generate, store, and serve up this data efficiently on the browser. Ben Weisburd on our team conceived a process that allowed us to extract reads for multiple variants from each individual sample, thus producing an individual mini-BAM—a file containing all the sequencing reads aligning to a specified list of variants in a given individual—for each available sample in gnomAD. (Not all samples in gnomAD had available read data, which is why some variants in v3.1 are missing read visualizations.)\nHe then combined these individual mini-BAMs into composite, multi-sample mini-BAMs comprised of data from 50 individuals each; replaced the original read group tags with hashes to anonymize the combined data; and recorded which genotypes and loci were covered in each 50-sample composite file. (Read group tags were replaced as a precaution against re-identification of individuals included in each multi-sample mini-BAM: read group tags, which are assigned to a given sequencing run, might otherwise be used to re-link genotypes across the genome for a specific individual. After replacing tags with hashes, the genotype data stored in a composite mini-BAM is completely unlinked to individuals.)\nThe browser queries a database that contains an index of genotype hashes, loci, and composite filepaths: each time you pull up a variant page with available read visualizations, the browser queries this database to discover which composite file to load into IGV and which unique read group hash to use to extract the reads specific to the genotype you’re viewing.\nWith this carefully engineered system in place, we’re excited to make read data visualizations for non-coding variants available for the very first time.\nSubsets\nBy popular request, we’ve computed allele counts and allele frequencies for major subsets of gnomAD v3.1, as follows:\nNon-v2: only genome samples that are new to the v3 or v3.1 release and not included in the v2 genomes\nNon-TOPMed: only samples that are not present in the Trans-Omics for Precision Medicine (TOPMed)/BRAVO release. The allele counts in this subset can thus be added to those of BRAVO to enable federated use of both datasets\nNon-cancer: only samples from individuals who were not ascertained for having cancer in a cancer study\nControls and biobanks: only samples collected specifically as controls for disease studies, or samples belonging to biobanks (e.g. BioMe, Genizon) or general population studies (e.g., 1000 Genomes, HGDP, PAGE)\nNon-neuro: only samples that were not collected as part of a neurologic or psychiatric case/control study, or samples collected as part of a neurologic or psychiatric case/control study but designated as controls\nSubset-specific allele counts and allele frequencies are available on variant pages by clicking on the drop-down menu at the top-right corner of the page:\nViewers will note that in addition to the gnomAD population frequencies, population frequencies specific to the HGDP and 1000 Genomes cohorts are available:\nWe provided allele counts and population frequencies for HGDP and 1000 Genomes to allow users to observe and compare the distribution of variant alleles across a diverse range of distinct human populations.\nIn-silico prediction annotations\nWe have added a number of variant annotations to help users interpret the clinical and functional effects of variants without needing to navigate to other websites and databases:\nREVEL\n, an ensemble score for predicting the pathogenicity of missense variants (based on 13 other variant predictors)\nCADD\nv1.6, a score predicting deleteriousness for both SNVs and indels\nSpliceAI\n, an deep learning predictor for variant effects on splicing\nPrimateAI\n, a deep neural network-trained score for classifying pathogenicity of missense mutations\nWe would particularly like to thank Kishore Jaganathan and Kyle Farh at Illumina for their material assistance in generating SpliceAI annotations for v3.1 variants.\nTweaks and updates\nThis latest release uses an updated version (\nv101\n) of the Variant Effect Predictor (VEP) based on the most recent Gencode\nv35\ngene models to provide consequence annotations. We have also updated the dbSNP release (\nb154\n) we use to annotate reference SNP identification (rsID) numbers. Users have alerted us to occasional rsID annotation errors in previous releases, which were caused by our pipeline matching rsIDs by chromosomal locus only. We now annotate rsIDs using both locus and allele information, which should reduce errors and ambiguities in annotation.\nAnother error mode identified in our previous releases is a technical artifact wherein homozygous alternate genotypes (particularly at long indel sites) were called as heterozygous genotypes despite having very high allele balances characteristic of homozygous alternate genotypes. This problem, described in greater detail\nhere\n, is caused by sample contamination, which can introduce stray reads supporting a reference allele at a site where the majority of reads contain an alternate allele. The genotype likelihood model used by our variant caller (GATK HaplotypeCaller) did not handle this condition properly.\nThe issue is now fixed for new gVCFs generated by HaplotypeCaller moving forward. However, for this release, the majority of variant calls were already generated using an uncorrected version of HaplotypeCaller. We therefore made an adjustment to heterozygous genotypes with highly skewed allele balance (>0.9) at common variant sites (>0.01 AF), setting these genotypes to homozygous alternate genotypes. This change is reflected in the aggregate allele frequencies, allele numbers, and allele counts displayed on the browser as well as in the genotype data released for the gnomAD HGDP and 1000 Genomes callset.\nFinally, we have changed the labels we use to classify individuals by chromosomal sex from “male” and “female” to “XY” and “XX,” respectively. While we have always used the terms “male” and “female” to refer to an individual’s chromosomal sex and not to gender, we recognize that this terminology is overloaded and could cause confusion to users. We also note that the terms “male” and “female,” when referring to chromosomal sex, can be applied to individuals with sex chromosomal aneuploidies, such as 47,XYY or 45,X. Since we remove samples with sex chromosomal aneuploidies from gnomAD during the QC process, we felt the most straightforward sex classification labels were “XX” and “XY.” This change is now reflected in both the v3.1 download files and in the browser.\nFree data access on multiple cloud providers\nOne recent development we’re excited to share with users is the availability of all gnomAD data on three cloud providers: Amazon Web Services, Microsoft Azure, and Google Cloud. All VCFs and Hail Tables from each gnomAD release are now available to download or read for free from each of these providers. Working in partnership with their public data hosting programs, we hope to encourage an even wider range of individuals and institutions to make use of gnomAD data for innovative research in human genetics and for the development of translational tools and medicines to treat and cure disease. For more details on how to access gnomAD through these cloud providers, read our blog post\nhere\n.\nSample and variant quality control\nThe overall approach to both sample and variant QC were very similar to the methods used for QC of\ngnomAD v3\n.\nSample QC hard filtering\nSample QC metrics were computed using Hail’s\nsample_qc()\nmodule on all autosomal bi-allelic SNVs, and outliers were filtered using the following cutoffs:\nNumber of SNVs: < 2.4M or > 3.75M\nNumber of singletons: > 100k\nRatio of heterozygous to homozygous variants: > 3.3\nHard filtering using BAM-level metrics was performed when such metrics were available. We removed samples that were outliers for:\nContamination: > 5%\nChimeras: > 5%\nMedian insert size: < 250\nAdditionally, samples were filtered if they had a mean coverage on chromosome 20 < 15X.\nSex inference\nThe process of imputing sex was the same as the method used for gnomAD v3, with slightly modified cutoffs. In previous releases, we used cutoffs on F-stat to determine XX and XY. The current pipeline uses a rough F-stat cutoff of 0.5 to split samples into the XX and XY categories. The final X and Y ploidy cutoffs are then determined from the means and standard deviations of those XX and XY distributions. Sex was assigned based on the following cutoffs:\nXY:\nnormalized X coverage < 1.29 &\nnormalized Y coverage > 0.1 &\nnormalized Y coverage < 1.16\nXX:\nnormalized X coverage > 1.45 &\nnormalized X coverage < 2.4 &\nnormalized Y coverage < 0.1\nThe plot below overlays the new v3.1 samples onto the normalized X coverage vs. normalized Y coverage plot with gnomAD v3 samples displayed in grey.\nAncestry inference\nThe method for ancestry assignment was similar to the method used for v3, with some slight changes in parameters and evaluation. 30 principal components (PCs) were computed using principal components analysis (PCA) as implemented by the\nhwe_normalized_pca()\nHail function on 76,419 high-quality variants selected as follows:\nWe took all sites that were used for gnomAD v2.1 and lifted them over to GRCh38\nWe added\n~5k sites\nwidely used for quality control of GWAS data defined by Shaun Purcell and lifted these sites over to GRCh38\nFrom these two sets of sites, we then selected all bi-allelic SNVs with an Inbreeding coefficient > -0.25 (no excess of heterozygotes)\nThe 30 PCs were visually inspected to determine which PCs were capturing variance explained by available ”known” population labels, which are obtained from a variety of sources, including labels captured by prior research studies as well as individual self-reported population labels. We determined that the first 16 PCs captured global ancestry variation with a clear drop in information content for higher PCs. We then trained a random forests classifier using those first 16 PCs as features on the samples with “known” population labels. Many of the new samples in v3.1 had “known” population labels, increasing the total samples used for training from 32,955 in v3 to 36,882 for this release. Ancestry labels were then assigned to all samples for which the random forest probability of that population assignment was > 75% according to the random forests model, and all remaining samples were given a population label of “other” (oth). The 75% cutoff was determined by holding back 20% of the samples with “known” population labels for evaluation of the model. We want to expressly acknowledge that this method of global ancestry assignment using self-reported population labels as a training set does not account for differences between genetic ancestry and self-reported population labels that are\nknown to exist\n.\nThe figure below shows a 2D uniform manifold approximation and projection\n(UMAP)\nof ancestry principal components 1-6 and 8-16, colored by the inferred ancestries of the samples. Note that long-range distances in this projection do not reflect genetic distance between populations.\nSample QC metric outlier filtering\nAs with gnomAD v3, we used the approach of determining sample QC outliers by computing sample QC metrics using the Hail\nsample_qc()\nmodule and regressing out the first 8 ancestry assignment PCs. Then we filtered samples that fell outside 4 median absolute deviations (MADs) from the median for the following metrics:\nn_snp\n,\nr_ti_tv\n,\nr_insertion_deletion\n,\nn_insertion\n,\nn_deletion\n,\nn_het\n,\nn_hom_var\n,\nn_transition\n, and\nn_transversion\n. Additionally, we filtered samples over 8 MADs above the median\nn_singleton\nmetric and over 4 MADs above the median\nr_het_hom_var\nmetric.\nVariant QC\nAs with gnomAD v3, we performed variant QC using the allele-specific version of GATK Variant Quality Score Recalibration\n(VQSR)\nwith the following features:\nSNVs:\nAS_FS\n,\nAS_SOR\n,\nAS_ReadPosRankSum\n,\nAS_MQRankSum\n,\nAS_QD\n,\nAS_MQ\nIndels:\nAS_FS\n,\nAS_SOR\n,\nAS_ReadPosRankSum\n,\nAS_MQRankSum\n,\nAS_QD\nWe used the standard GATK training resources (HapMap, Omni, 1000 Genomes, Mills indels) in addition to ~19M transmitted singletons (alleles confidently observed exactly twice in gnomAD, once in a parent and once in a child) from 6,743 trios present in the raw data.\nThe figure below shows the precision and recall curves for the allele-specific VQSR model (AS_VQSR_TS) using a truth sample (\nNA12878\n) present in our data. The lines at 90 (SNVs) and 80 (indels) indicate the cutoffs we chose for filtering.\nThe same variant hard filters were applied:\nNo high-quality genotype (GQ>=20, DP>=10, and AB>=0.2 for heterozygotes) called for the variant\nInbreedingCoeff < -0.3\nThese filtering criteria excluded 12.2% of SNVs and 32.5% of indels, resulting in 569,860,911 SNVs and 74,407,067 indels that passed all filters in the v3.1 release.\nUpdates:\nAugust 14, 2023 to correct the number of 1000 Genomes Project samples from 2,435 to 2,500.\nDecember 17, 2021 to provide additional details to the\nancestry inference section\nof this post."
    },
    {
        "title": "gnomAD v3.1",
        "link": "https://gnomad.broadinstitute.org/news/2020-10-gnomad-v3-1/",
        "date": "2020-10-29",
        "text": "Today, the gnomAD Production Team is proud to announce the release of gnomAD v3.1, an update to our\nprevious genome release\n. The v3.1 data set adds 4,454 genomes, bringing the total to 76,156 whole genomes mapped to the GRCh38 reference sequence. (Our most recent exome release is available in\ngnomAD v2.1\n.)\nDespite the minor numbering of this release, we bring you an update filled with firsts.\nFor the first time\n, we:\nProvide individual genotypes in addition to variant calls for a subset of gnomAD. This highly diverse subset includes new data from >60 distinct populations from Africa, Europe, the Middle East, South and Central Asia, East Asia, Oceania, and the Americas\nProvide and display data from samples of Middle Eastern ancestry\nDisplay read data visualizations for non-coding variants—an effort that required the generation of visualizations for over 2.5 billion genotypes observed in this release\nDisplay manual curations for predicted loss-of-function variants on the gnomAD browser\nGenerated the dataset by incrementally adding new samples onto an already-existing callset, eliminating the time and cost typically required to re-call existing samples\nMake all gnomAD data—for this release as well as previous releases—freely available for download or export on three cloud providers: Amazon Web Services, Microsoft Azure, and Google Cloud\nAnd we’re currently polishing up the final touches on our first-ever mitochondrial variant release on v3.1, which will be coming very soon.\nThese new features, along with further details regarding the production of the v3.1 release, are described in the following blog posts:\ngnomAD v3.1 New Content, Methods, Annotations, and Data Availability\nLoss-of-Function Curations in gnomAD\nOpen access to gnomAD data on multiple cloud providers\nWe hope these new features will prove useful to our users, and we welcome feedback on the latest release at our email address:\ngnomad@broadinstitute.org\n.\nAcknowledgements\nWe wish to acknowledge the extraordinary and extensive network of supporters and contributors who have made this release possible, beginning with the gnomAD Consortium of 143 principal investigators, whose willingness to share data is the cornerstone of all our efforts.\nAdditionally, we thank the Broad Genomics Platform for their continued, material support in sequencing, storing, and managing tens of thousands of genome samples included in this release; Trevyn Langsford, Nareh Sahakian, and Charlotte Tolonen in the Broad Data Sciences Platform for their help re-processing a critical cohort of externally-sequenced samples to the functional equivalence standard; Alicia Martin for help obtaining data from the Human Genome Diversity Project and parsing the relevant population labels; Chris Vittal for developing and executing the novel incremental joint calling procedure we used to create the v3.1 callset in record time (and on a record budget); Kat Tarasova for truly heroic, behind-the-scenes work ironing out sample permissions and metadata; Ben Weisburd for his resourceful and creative pipeline design enabling read visualizations for >2.5 billion individual genotypes; Laurent Francioli for his expert opinions on analysis and detailed code review; Julia Goodrich for executing the lion’s share of the quality control and production of the nuclear variant callset; Michael Wilson and William Phu for their essential work helping to produce the nuclear variant callset; Katherine Chao for important code contributions to the nuclear variant production pipeline; Moriel Singer-Berk, Eleanor Seaby, Eleina England, Dan Rhodes, Rachel Son, and Emily Evangelista for their contributions to the pLoF curation; Kishore Jaganathan and Kyle Farh at Illumina for generating and making SpliceAI scores available to our users; the Hail team for timely and essential help troubleshooting runtime issues and pipeline bugs; Nick Watts and Matt Solomonson for their brilliant and beautiful design of the gnomAD browser and for their meticulous and ongoing labors to maintain it; Grace Tiao for orchestrating and overseeing the many moving parts of the release; Anne O’Donnell-Luria, Konrad Karczewksi, Ben Neale, Mike Talkowski, Daniel MacArthur, Mark Daly, and Heidi Rehm for moral support, advice, and suggestions regarding callset generation, quality control, browser design, and scientific directions; and Eric Banks, Anthony Philippakis, Danielle Ciofani, and Cotton Seed for their assistance in making our data freely available on multiple cloud providers to the benefit of our many users around the world."
    },
    {
        "title": "Loss-of-Function Curations in gnomAD",
        "link": "https://gnomad.broadinstitute.org/news/2020-10-loss-of-function-curations-in-gnomad/",
        "date": "2020-10-29",
        "text": "Today we are pleased to announce the incorporation of manual loss-of-function (LoF) curations into the gnomAD v2.1.1 browser. As of this release, we have curated all homozygous pLoFs and a small set of recessive genes (e.g.,\nGAA\n,\nGLA\n,\nIDUA\n,\nSMPD1\n,\nGBA\n,\nFIG4\n,\nMCOLN1\n,\nAP4B1\n,\nAP4M1\n,\nAP4S1\n, and\nAP4E1\n). These curations were performed for multiple projects including the recently published work,\nKarczewski et al. 2020 Nature\n, as well as other gene-specific projects. We are so excited to start sharing this data with you that we are including it in the gnomAD v3.1 release announcement but really these are a new gnomAD v2.1.1 feature at the moment. More datasets will be added to the browser as they are completed.\nWhat is loss-of-function curation?\nThe classification of LoF variants is a result of a specialized and manual curation of predicted loss-of-function (pLoF) variants that have passed all\nLOFTEE\nfilters and other QC flags in gnomAD to determine how likely these variants are to result in loss of function. After reviewing all the high-quality homozygous pLoF variants in gnomAD, we determined that 28% may not actually result in loss-of-function, highlighting the need for careful curation of these variants. Curation results also correlated well with our biological expectation; for heterozygous variants, 60% of pLoF variants in a set of haploinsufficient disease genes—versus 25% in a set of recessive disease genes—did not appear to result in loss-of-function.\npLoF curated variants were assigned one of five classifications based on the presence or absence of certain error modes. These classifications include: LoF, likely LoF, uncertain LoF, likely not LoF, and not LoF. For each curated variant, two biocurators performed an independent curation, and discrepancies between biocurators were discussed and resolved as a team.\nVariants classified as “not LoF” have error modes that indicate these variants are predicted to not result in loss of function, while “LoF” classified variants have no error modes (or at most minor error modes) that indicate they are predicted to result in loss-of-function. Similar to the use of the “likely” qualifier in the\nACMG/AMP criteria for sequence variant interpretation\nfor classifying likely pathogenic and likely benign variants, “likely LoF” and “likely not LoF” classified variants are slightly less confidently predicted to result in loss-of-function or not loss-of-function, respectively. Variants with an “uncertain LoF” classification are similar to the ACMG/AMP variants of uncertain significance (VUS’s), and do not have sufficient evidence to point towards a definitive classification. We recommend using these classifications in coordination with applying PVS1 in the ACMG/AMP criteria for sequence variant interpretation.\nHow to use these annotations\nFor “likely not LoF” and “not LoF” variants, our extensive manual curation supports the conclusion that these variants may either be the result of a technical sequencing error, or their predicted biological effect is not loss-of-function. For those variants with technical errors that are also deemed “not LoF,” the allele frequency of these variants in gnomAD should not be considered the true frequency. This means that if you observe (and have confirmed) the variant in a patient, you should not use the gnomAD allele frequencies to estimate the prevalence of this variant — particularly for cases where there is a flag suggesting that the variant in gnomAD appears to be an artifact in at least some of the individuals.\nFor “likely LoF” and “LoF” variants, our manual curation provides increased confidence that these variants are truly loss-of-function variants and are expected to result in nonsense-mediated decay or an otherwise null effect. This systematically excludes a number of common annotation and sequencing error modes. Ultimately, however, functional studies will be needed to fully validate the potential loss-of-function impact of a variant.\n“Uncertain LoF” variants represent cases where we were unable to reach a more conclusive classification and therefore should not be interpreted as falling into either of the above two categories.\nLoss-of-function curations in the gnomAD browser\nThese curations are viewable from both the gene and variant pages on the browser. Within the gene page there is a new column that lists the curation result for all variants for which curation is available. If you hover over the curation result, you will see the error flag(s) that were identified during the curation of that variant. To date, curation has been performed on over 4,000 high-confidence pLoF variants in the gnomAD dataset, but this is a small fraction of the 444,000 total pLoF variants. As additional curations are performed, we will continue to update the gnomAD browser.\nOn the variant page there is an LoF curation section that also lists the flag(s) present during the curation of this variant with the verdict it received as a result of the curation (large red arrow). A popup (small red arrow) is available for a detailed explanation of these error modes from the variant page as well.\nCaveats\nThese curations are based on the presence of these variants in the gnomAD database and do not take into account the presence of the variant in individuals with documented disease. We view LoF curation as an important step to be taken to evaluate whether a variant is expected to result in loss-of-function and would follow this with curation for the clinical impact of a variant (e.g., pathogenic, benign, etc.) following the ACMG/AMP standards and guidelines for the interpretation of sequence variants (Richards et al. 2015), including whether loss-of-function is a mechanism of disease for the specific gene. Therefore, even if a variant is classified as “LoF” or “likely LoF,” it may not meet criteria for pathogenicity with respect to human disease.\nAdditionally, the gnomAD LoF curation of a variant is not in any way determined based on functional evidence. Rather, predictions are entirely based on manual interpretation of splicing, conservation, computational annotations, and sequence data quality in gnomAD and the UCSC genome browser.\nAcknowledgements\nWe would like to thank all the people who have been critical in the completion of these projects. This includes an amazing group of biocurators, especially Eleanor Seaby, Eleina England, Dan Rhodes, Rachel Son, and Emily Evangelista. In addition, the curations could not have been completed without the support of software engineers Nick Watts, Ben Weisburd, and Matthew Solomonson. All of these projects were done in collaboration with gnomAD team members, including Konrad Karczewski, Beryl Cummings, Miriam Udler, and Julia Goodrich. Lastly, we want to acknowledge the support and guidance of this work by Samantha Baxter, Daniel MacArthur, and Heidi Rehm."
    }
]